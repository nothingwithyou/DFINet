{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497dc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "from munch import Munch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from core.model import build_model\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import argparse\n",
    "from core.data_loader import get_train_loader\n",
    "from core.data_loader import get_test_loader\n",
    "from core.solver import Solver, compute_d_loss\n",
    "from core.data_loader import InputFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8efd33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# model arguments\n",
    "parser.add_argument('--img_size', type=int, default=256,\n",
    "                    help='Image resolution')\n",
    "parser.add_argument('--num_domains', type=int, default=2,\n",
    "                    help='Number of domains')\n",
    "parser.add_argument('--latent_dim', type=int, default=16,\n",
    "                    help='Latent vector dimension')\n",
    "parser.add_argument('--latent_channels', type=int, default=64,\n",
    "                    help='latent channels for coarse')\n",
    "parser.add_argument('--norm', type=str, default='in',\n",
    "                    help='normalization type for coarse')\n",
    "parser.add_argument('--pad_type', type=str, default='zero',\n",
    "                    help='the padding type for coarse')\n",
    "parser.add_argument('--activation', type=str, default='elu',\n",
    "                    help='the activation type for coarse')  # elu\n",
    "parser.add_argument('--hidden_dim', type=int, default=512,\n",
    "                    help='Hidden dimension of mapping network')\n",
    "parser.add_argument('--style_dim', type=int, default=64,\n",
    "                    help='Style code dimension')\n",
    "\n",
    "# weight for objective functions\n",
    "parser.add_argument('--lambda_reg', type=float, default=1,\n",
    "                    help='Weight for R1 regularization')\n",
    "parser.add_argument('--lambda_cyc', type=float, default=2,\n",
    "                    help='Weight for cyclic consistency loss')\n",
    "parser.add_argument('--lambda_sty', type=float, default=1,\n",
    "                    help='Weight for style reconstruction loss')\n",
    "parser.add_argument('--lambda_ds', type=float, default=1,\n",
    "                    help='Weight for diversity sensitive loss')\n",
    "parser.add_argument('--lambda_pl1', type=float, default=1, help='the parameter of parsing L1Loss')\n",
    "parser.add_argument('--lambda_il1', type=float, default=1, help='the parameter of image L1Loss')\n",
    "parser.add_argument('--lambda_perceptual', type=float, default=2,\n",
    "                    help='the parameter of FML1Loss (perceptual loss)')\n",
    "parser.add_argument('--lambda_gan', type=float, default=1,\n",
    "                    help='the parameter of valid loss of AdaReconL1Loss; 0 is recommended')\n",
    "parser.add_argument('--s_loss', type=float, default=2.5, help='STYLE_LOSS_WEIGHT')\n",
    "parser.add_argument('--p_loss', type=float, default=0.1, help='CONTENT_LOSS_WEIGHT')\n",
    "parser.add_argument('--ds_iter', type=int, default=100000,\n",
    "                    help='Number of iterations to optimize diversity sensitive loss')\n",
    "parser.add_argument('--w_hpf', type=float, default=1,\n",
    "                    help='weight for high-pass filtering')\n",
    "\n",
    "# training arguments\n",
    "parser.add_argument('--randcrop_prob', type=float, default=0.5,\n",
    "                    help='Probabilty of using random-resized cropping')\n",
    "parser.add_argument('--total_iters', type=int, default=100000,\n",
    "                    help='Number of total iterations')\n",
    "parser.add_argument('--resume_iter', type=int, default=0,\n",
    "                    help='Iterations to resume training/testing')\n",
    "parser.add_argument('--batch_size', type=int, default=8,\n",
    "                    help='Batch size for training')\n",
    "parser.add_argument('--val_batch_size', type=int, default=32,\n",
    "                    help='Batch size for validation')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='Learning rate for D, E and G')\n",
    "parser.add_argument('--f_lr', type=float, default=1e-6,\n",
    "                    help='Learning rate for F')\n",
    "parser.add_argument('--beta1', type=float, default=0.0,\n",
    "                    help='Decay rate for 1st moment of Adam')\n",
    "parser.add_argument('--beta2', type=float, default=0.99,\n",
    "                    help='Decay rate for 2nd moment of Adam')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                    help='Weight decay for optimizer')\n",
    "parser.add_argument('--num_outs_per_domain', type=int, default=10,\n",
    "                    help='Number of generated images per domain during sampling')\n",
    "\n",
    "# misc\n",
    "parser.add_argument('--mode', type=str, required=True,\n",
    "                    choices=['train', 'eval', 'sample', 'pre_train'],\n",
    "                    help='This argument is used in solver')\n",
    "parser.add_argument('--num_workers', type=int, default=4,\n",
    "                    help='Number of workers used in DataLoader')\n",
    "parser.add_argument('--seed', type=int, default=777,\n",
    "                    help='Seed for random number generator')\n",
    "\n",
    "# directory for training\n",
    "parser.add_argument('--train_img_dir', type=str, default='data/train',\n",
    "                    help='Directory containing training images')\n",
    "parser.add_argument('--val_img_dir', type=str, default='data/val',\n",
    "                    help='Directory containing validation images')\n",
    "parser.add_argument('--sample_dir', type=str, default='amples',\n",
    "                    help='Directory for saving generated images')\n",
    "parser.add_argument('--checkpoint_dir', type=str, default='checkpoints',\n",
    "                    help='Directory for saving network checkpoints')\n",
    "\n",
    "# directory for calculating metrics\n",
    "parser.add_argument('--eval_dir', type=str, default='eval',\n",
    "                    help='Directory for saving metrics, i.e., FID and LPIPS')\n",
    "\n",
    "# directory for testing\n",
    "parser.add_argument('--result_dir', type=str, default='results',\n",
    "                    help='Directory for saving generated images and videos')\n",
    "parser.add_argument('--src_dir', type=str, default='assets/representative/celeba_hq/src',\n",
    "                    help='Directory containing input source images')\n",
    "parser.add_argument('--ref_dir', type=str, default='assets/representative/celeba_hq/ref',\n",
    "                    help='Directory containing input reference images')\n",
    "parser.add_argument('--inp_dir', type=str, default='assets/representative/custom/female',\n",
    "                    help='input directory when aligning faces')\n",
    "parser.add_argument('--out_dir', type=str, default='assets/representative/celeba_hq/src/female',\n",
    "                    help='output directory when aligning faces')\n",
    "\n",
    "# face alignment\n",
    "parser.add_argument('--wing_path', type=str, default='checkpoints/wing.ckpt')\n",
    "parser.add_argument('--lm_path', type=str, default='checkpoints/celeba_lm_mean.npz')\n",
    "\n",
    "# step size\n",
    "parser.add_argument('--print_every', type=int, default=10)\n",
    "parser.add_argument('--sample_every', type=int, default=5000)\n",
    "parser.add_argument('--save_every', type=int, default=10000)\n",
    "parser.add_argument('--eval_every', type=int, default=50000)\n",
    "\n",
    "args = parser.parse_args(['--mode','pre_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37891f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataLoader to fetch source images during the training phase...\n",
      "Preparing DataLoader for the generation phase...\n"
     ]
    }
   ],
   "source": [
    "pre_loaders = Munch(src=get_train_loader(root=args.train_img_dir,\n",
    "                                             which='source',\n",
    "                                             img_size=args.img_size,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             prob=args.randcrop_prob,\n",
    "                                             num_workers=args.num_workers,\n",
    "                                             pre=True),\n",
    "                        val=get_test_loader(root=args.val_img_dir,\n",
    "                                            img_size=args.img_size,\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c534527",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = InputFetcher(pre_loaders.src, latent_dim=args.latent_dim, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00197c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher_val = InputFetcher(pre_loaders.val, None, latent_dim=args.latent_dim, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7972774",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_val = next(fetcher_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da75777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14051e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = build_model(args,pre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993f15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = torch.load('./checkpoints/parsing.ckpt')\n",
    "model2_dict = nets.generator.module.state_dict()\n",
    "state_dict = {k:v for k,v in pre_model.items() if k in model2_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4472ab8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['from_rgb.conv2d.module.bias', 'from_rgb.conv2d.module.weight_u', 'from_rgb.conv2d.module.weight_v', 'from_rgb.conv2d.module.weight_bar', 'from_rgb.mask_conv2d.module.bias', 'from_rgb.mask_conv2d.module.weight_u', 'from_rgb.mask_conv2d.module.weight_v', 'from_rgb.mask_conv2d.module.weight_bar', 'encode.0.conv1.conv2d.module.bias', 'encode.0.conv1.conv2d.module.weight_u', 'encode.0.conv1.conv2d.module.weight_v', 'encode.0.conv1.conv2d.module.weight_bar', 'encode.0.conv1.mask_conv2d.module.bias', 'encode.0.conv1.mask_conv2d.module.weight_u', 'encode.0.conv1.mask_conv2d.module.weight_v', 'encode.0.conv1.mask_conv2d.module.weight_bar', 'encode.0.conv2.conv2d.module.bias', 'encode.0.conv2.conv2d.module.weight_u', 'encode.0.conv2.conv2d.module.weight_v', 'encode.0.conv2.conv2d.module.weight_bar', 'encode.0.conv2.mask_conv2d.module.bias', 'encode.0.conv2.mask_conv2d.module.weight_u', 'encode.0.conv2.mask_conv2d.module.weight_v', 'encode.0.conv2.mask_conv2d.module.weight_bar', 'encode.0.norm1.weight', 'encode.0.norm1.bias', 'encode.0.norm2.weight', 'encode.0.norm2.bias', 'encode.0.conv1x1.weight', 'encode.1.conv1.conv2d.module.bias', 'encode.1.conv1.conv2d.module.weight_u', 'encode.1.conv1.conv2d.module.weight_v', 'encode.1.conv1.conv2d.module.weight_bar', 'encode.1.conv1.mask_conv2d.module.bias', 'encode.1.conv1.mask_conv2d.module.weight_u', 'encode.1.conv1.mask_conv2d.module.weight_v', 'encode.1.conv1.mask_conv2d.module.weight_bar', 'encode.1.conv2.conv2d.module.bias', 'encode.1.conv2.conv2d.module.weight_u', 'encode.1.conv2.conv2d.module.weight_v', 'encode.1.conv2.conv2d.module.weight_bar', 'encode.1.conv2.mask_conv2d.module.bias', 'encode.1.conv2.mask_conv2d.module.weight_u', 'encode.1.conv2.mask_conv2d.module.weight_v', 'encode.1.conv2.mask_conv2d.module.weight_bar', 'encode.1.norm1.weight', 'encode.1.norm1.bias', 'encode.1.norm2.weight', 'encode.1.norm2.bias', 'encode.1.conv1x1.weight', 'encode.2.conv1.conv2d.module.bias', 'encode.2.conv1.conv2d.module.weight_u', 'encode.2.conv1.conv2d.module.weight_v', 'encode.2.conv1.conv2d.module.weight_bar', 'encode.2.conv1.mask_conv2d.module.bias', 'encode.2.conv1.mask_conv2d.module.weight_u', 'encode.2.conv1.mask_conv2d.module.weight_v', 'encode.2.conv1.mask_conv2d.module.weight_bar', 'encode.2.conv2.conv2d.module.bias', 'encode.2.conv2.conv2d.module.weight_u', 'encode.2.conv2.conv2d.module.weight_v', 'encode.2.conv2.conv2d.module.weight_bar', 'encode.2.conv2.mask_conv2d.module.bias', 'encode.2.conv2.mask_conv2d.module.weight_u', 'encode.2.conv2.mask_conv2d.module.weight_v', 'encode.2.conv2.mask_conv2d.module.weight_bar', 'encode.2.norm1.weight', 'encode.2.norm1.bias', 'encode.2.norm2.weight', 'encode.2.norm2.bias', 'encode.2.conv1x1.weight', 'encode.3.conv1.conv2d.module.bias', 'encode.3.conv1.conv2d.module.weight_u', 'encode.3.conv1.conv2d.module.weight_v', 'encode.3.conv1.conv2d.module.weight_bar', 'encode.3.conv1.mask_conv2d.module.bias', 'encode.3.conv1.mask_conv2d.module.weight_u', 'encode.3.conv1.mask_conv2d.module.weight_v', 'encode.3.conv1.mask_conv2d.module.weight_bar', 'encode.3.conv2.conv2d.module.bias', 'encode.3.conv2.conv2d.module.weight_u', 'encode.3.conv2.conv2d.module.weight_v', 'encode.3.conv2.conv2d.module.weight_bar', 'encode.3.conv2.mask_conv2d.module.bias', 'encode.3.conv2.mask_conv2d.module.weight_u', 'encode.3.conv2.mask_conv2d.module.weight_v', 'encode.3.conv2.mask_conv2d.module.weight_bar', 'encode.3.norm1.weight', 'encode.3.norm1.bias', 'encode.3.norm2.weight', 'encode.3.norm2.bias', 'encode.4.conv1.conv2d.module.bias', 'encode.4.conv1.conv2d.module.weight_u', 'encode.4.conv1.conv2d.module.weight_v', 'encode.4.conv1.conv2d.module.weight_bar', 'encode.4.conv1.mask_conv2d.module.bias', 'encode.4.conv1.mask_conv2d.module.weight_u', 'encode.4.conv1.mask_conv2d.module.weight_v', 'encode.4.conv1.mask_conv2d.module.weight_bar', 'encode.4.conv2.conv2d.module.bias', 'encode.4.conv2.conv2d.module.weight_u', 'encode.4.conv2.conv2d.module.weight_v', 'encode.4.conv2.conv2d.module.weight_bar', 'encode.4.conv2.mask_conv2d.module.bias', 'encode.4.conv2.mask_conv2d.module.weight_u', 'encode.4.conv2.mask_conv2d.module.weight_v', 'encode.4.conv2.mask_conv2d.module.weight_bar', 'encode.4.norm1.weight', 'encode.4.norm1.bias', 'encode.4.norm2.weight', 'encode.4.norm2.bias', 'encode.5.conv1.conv2d.module.bias', 'encode.5.conv1.conv2d.module.weight_u', 'encode.5.conv1.conv2d.module.weight_v', 'encode.5.conv1.conv2d.module.weight_bar', 'encode.5.conv1.mask_conv2d.module.bias', 'encode.5.conv1.mask_conv2d.module.weight_u', 'encode.5.conv1.mask_conv2d.module.weight_v', 'encode.5.conv1.mask_conv2d.module.weight_bar', 'encode.5.conv2.conv2d.module.bias', 'encode.5.conv2.conv2d.module.weight_u', 'encode.5.conv2.conv2d.module.weight_v', 'encode.5.conv2.conv2d.module.weight_bar', 'encode.5.conv2.mask_conv2d.module.bias', 'encode.5.conv2.mask_conv2d.module.weight_u', 'encode.5.conv2.mask_conv2d.module.weight_v', 'encode.5.conv2.mask_conv2d.module.weight_bar', 'encode.5.norm1.weight', 'encode.5.norm1.bias', 'encode.5.norm2.weight', 'encode.5.norm2.bias', 'encode.6.conv1.conv2d.module.bias', 'encode.6.conv1.conv2d.module.weight_u', 'encode.6.conv1.conv2d.module.weight_v', 'encode.6.conv1.conv2d.module.weight_bar', 'encode.6.conv1.mask_conv2d.module.bias', 'encode.6.conv1.mask_conv2d.module.weight_u', 'encode.6.conv1.mask_conv2d.module.weight_v', 'encode.6.conv1.mask_conv2d.module.weight_bar', 'encode.6.conv2.conv2d.module.bias', 'encode.6.conv2.conv2d.module.weight_u', 'encode.6.conv2.conv2d.module.weight_v', 'encode.6.conv2.conv2d.module.weight_bar', 'encode.6.conv2.mask_conv2d.module.bias', 'encode.6.conv2.mask_conv2d.module.weight_u', 'encode.6.conv2.mask_conv2d.module.weight_v', 'encode.6.conv2.mask_conv2d.module.weight_bar', 'encode.6.norm1.weight', 'encode.6.norm1.bias', 'encode.6.norm2.weight', 'encode.6.norm2.bias', 'bottleneck.0.conv2d.module.bias', 'bottleneck.0.conv2d.module.weight_u', 'bottleneck.0.conv2d.module.weight_v', 'bottleneck.0.conv2d.module.weight_bar', 'bottleneck.0.mask_conv2d.module.bias', 'bottleneck.0.mask_conv2d.module.weight_u', 'bottleneck.0.mask_conv2d.module.weight_v', 'bottleneck.0.mask_conv2d.module.weight_bar', 'bottleneck.1.conv2d.module.bias', 'bottleneck.1.conv2d.module.weight_u', 'bottleneck.1.conv2d.module.weight_v', 'bottleneck.1.conv2d.module.weight_bar', 'bottleneck.1.mask_conv2d.module.bias', 'bottleneck.1.mask_conv2d.module.weight_u', 'bottleneck.1.mask_conv2d.module.weight_v', 'bottleneck.1.mask_conv2d.module.weight_bar', 'bottleneck.2.conv2d.module.bias', 'bottleneck.2.conv2d.module.weight_u', 'bottleneck.2.conv2d.module.weight_v', 'bottleneck.2.conv2d.module.weight_bar', 'bottleneck.2.mask_conv2d.module.bias', 'bottleneck.2.mask_conv2d.module.weight_u', 'bottleneck.2.mask_conv2d.module.weight_v', 'bottleneck.2.mask_conv2d.module.weight_bar', 'bottleneck.3.conv2d.module.bias', 'bottleneck.3.conv2d.module.weight_u', 'bottleneck.3.conv2d.module.weight_v', 'bottleneck.3.conv2d.module.weight_bar', 'bottleneck.3.mask_conv2d.module.bias', 'bottleneck.3.mask_conv2d.module.weight_u', 'bottleneck.3.mask_conv2d.module.weight_v', 'bottleneck.3.mask_conv2d.module.weight_bar', 'decode.0.conv1.conv2d.module.bias', 'decode.0.conv1.conv2d.module.weight_u', 'decode.0.conv1.conv2d.module.weight_v', 'decode.0.conv1.conv2d.module.weight_bar', 'decode.0.conv1.mask_conv2d.module.bias', 'decode.0.conv1.mask_conv2d.module.weight_u', 'decode.0.conv1.mask_conv2d.module.weight_v', 'decode.0.conv1.mask_conv2d.module.weight_bar', 'decode.0.conv2.conv2d.module.bias', 'decode.0.conv2.conv2d.module.weight_u', 'decode.0.conv2.conv2d.module.weight_v', 'decode.0.conv2.conv2d.module.weight_bar', 'decode.0.conv2.mask_conv2d.module.bias', 'decode.0.conv2.mask_conv2d.module.weight_u', 'decode.0.conv2.mask_conv2d.module.weight_v', 'decode.0.conv2.mask_conv2d.module.weight_bar', 'decode.0.norm1.fc.weight', 'decode.0.norm1.fc.bias', 'decode.0.norm2.fc.weight', 'decode.0.norm2.fc.bias', 'decode.1.conv1.conv2d.module.bias', 'decode.1.conv1.conv2d.module.weight_u', 'decode.1.conv1.conv2d.module.weight_v', 'decode.1.conv1.conv2d.module.weight_bar', 'decode.1.conv1.mask_conv2d.module.bias', 'decode.1.conv1.mask_conv2d.module.weight_u', 'decode.1.conv1.mask_conv2d.module.weight_v', 'decode.1.conv1.mask_conv2d.module.weight_bar', 'decode.1.conv2.conv2d.module.bias', 'decode.1.conv2.conv2d.module.weight_u', 'decode.1.conv2.conv2d.module.weight_v', 'decode.1.conv2.conv2d.module.weight_bar', 'decode.1.conv2.mask_conv2d.module.bias', 'decode.1.conv2.mask_conv2d.module.weight_u', 'decode.1.conv2.mask_conv2d.module.weight_v', 'decode.1.conv2.mask_conv2d.module.weight_bar', 'decode.1.norm1.fc.weight', 'decode.1.norm1.fc.bias', 'decode.1.norm2.fc.weight', 'decode.1.norm2.fc.bias', 'decode.2.conv1.conv2d.module.bias', 'decode.2.conv1.conv2d.module.weight_u', 'decode.2.conv1.conv2d.module.weight_v', 'decode.2.conv1.conv2d.module.weight_bar', 'decode.2.conv1.mask_conv2d.module.bias', 'decode.2.conv1.mask_conv2d.module.weight_u', 'decode.2.conv1.mask_conv2d.module.weight_v', 'decode.2.conv1.mask_conv2d.module.weight_bar', 'decode.2.conv2.conv2d.module.bias', 'decode.2.conv2.conv2d.module.weight_u', 'decode.2.conv2.conv2d.module.weight_v', 'decode.2.conv2.conv2d.module.weight_bar', 'decode.2.conv2.mask_conv2d.module.bias', 'decode.2.conv2.mask_conv2d.module.weight_u', 'decode.2.conv2.mask_conv2d.module.weight_v', 'decode.2.conv2.mask_conv2d.module.weight_bar', 'decode.2.norm1.fc.weight', 'decode.2.norm1.fc.bias', 'decode.2.norm2.fc.weight', 'decode.2.norm2.fc.bias', 'decode.3.conv1.conv2d.module.bias', 'decode.3.conv1.conv2d.module.weight_u', 'decode.3.conv1.conv2d.module.weight_v', 'decode.3.conv1.conv2d.module.weight_bar', 'decode.3.conv1.mask_conv2d.module.bias', 'decode.3.conv1.mask_conv2d.module.weight_u', 'decode.3.conv1.mask_conv2d.module.weight_v', 'decode.3.conv1.mask_conv2d.module.weight_bar', 'decode.3.conv2.conv2d.module.bias', 'decode.3.conv2.conv2d.module.weight_u', 'decode.3.conv2.conv2d.module.weight_v', 'decode.3.conv2.conv2d.module.weight_bar', 'decode.3.conv2.mask_conv2d.module.bias', 'decode.3.conv2.mask_conv2d.module.weight_u', 'decode.3.conv2.mask_conv2d.module.weight_v', 'decode.3.conv2.mask_conv2d.module.weight_bar', 'decode.3.norm1.fc.weight', 'decode.3.norm1.fc.bias', 'decode.3.norm2.fc.weight', 'decode.3.norm2.fc.bias', 'decode.4.conv1.conv2d.module.bias', 'decode.4.conv1.conv2d.module.weight_u', 'decode.4.conv1.conv2d.module.weight_v', 'decode.4.conv1.conv2d.module.weight_bar', 'decode.4.conv1.mask_conv2d.module.bias', 'decode.4.conv1.mask_conv2d.module.weight_u', 'decode.4.conv1.mask_conv2d.module.weight_v', 'decode.4.conv1.mask_conv2d.module.weight_bar', 'decode.4.conv2.conv2d.module.bias', 'decode.4.conv2.conv2d.module.weight_u', 'decode.4.conv2.conv2d.module.weight_v', 'decode.4.conv2.conv2d.module.weight_bar', 'decode.4.conv2.mask_conv2d.module.bias', 'decode.4.conv2.mask_conv2d.module.weight_u', 'decode.4.conv2.mask_conv2d.module.weight_v', 'decode.4.conv2.mask_conv2d.module.weight_bar', 'decode.4.norm1.fc.weight', 'decode.4.norm1.fc.bias', 'decode.4.norm2.fc.weight', 'decode.4.norm2.fc.bias', 'decode.4.conv1x1.weight', 'decode.5.conv1.conv2d.module.bias', 'decode.5.conv1.conv2d.module.weight_u', 'decode.5.conv1.conv2d.module.weight_v', 'decode.5.conv1.conv2d.module.weight_bar', 'decode.5.conv1.mask_conv2d.module.bias', 'decode.5.conv1.mask_conv2d.module.weight_u', 'decode.5.conv1.mask_conv2d.module.weight_v', 'decode.5.conv1.mask_conv2d.module.weight_bar', 'decode.5.conv2.conv2d.module.bias', 'decode.5.conv2.conv2d.module.weight_u', 'decode.5.conv2.conv2d.module.weight_v', 'decode.5.conv2.conv2d.module.weight_bar', 'decode.5.conv2.mask_conv2d.module.bias', 'decode.5.conv2.mask_conv2d.module.weight_u', 'decode.5.conv2.mask_conv2d.module.weight_v', 'decode.5.conv2.mask_conv2d.module.weight_bar', 'decode.5.norm1.fc.weight', 'decode.5.norm1.fc.bias', 'decode.5.norm2.fc.weight', 'decode.5.norm2.fc.bias', 'decode.5.conv1x1.weight', 'decode.6.conv1.conv2d.module.bias', 'decode.6.conv1.conv2d.module.weight_u', 'decode.6.conv1.conv2d.module.weight_v', 'decode.6.conv1.conv2d.module.weight_bar', 'decode.6.conv1.mask_conv2d.module.bias', 'decode.6.conv1.mask_conv2d.module.weight_u', 'decode.6.conv1.mask_conv2d.module.weight_v', 'decode.6.conv1.mask_conv2d.module.weight_bar', 'decode.6.conv2.conv2d.module.bias', 'decode.6.conv2.conv2d.module.weight_u', 'decode.6.conv2.conv2d.module.weight_v', 'decode.6.conv2.conv2d.module.weight_bar', 'decode.6.conv2.mask_conv2d.module.bias', 'decode.6.conv2.mask_conv2d.module.weight_u', 'decode.6.conv2.mask_conv2d.module.weight_v', 'decode.6.conv2.mask_conv2d.module.weight_bar', 'decode.6.norm1.fc.weight', 'decode.6.norm1.fc.bias', 'decode.6.norm2.fc.weight', 'decode.6.norm2.fc.bias', 'decode.6.conv1x1.weight', 'to_rgb.0.weight', 'to_rgb.0.bias', 'to_rgb.2.weight', 'to_rgb.2.bias', 'hpf.filter'], unexpected_keys=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nets.generator.module.load_state_dict(pre_model, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad805f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Generator' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cda7a136c034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints/parsing.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 948\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Generator' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "nets.generator.module.load_state_dict('./checkpoints/parsing.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ebdb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real, x_p, y_org, mask = inputs_val.x, inputs_val.p, inputs_val.y, inputs_val.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79087cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50614e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_d_loss(nets, args, x_real, x_p, y_org, y_trg=None, z_trg=None, x_ref=None, masks=None, pre=False):\n",
    "    if not pre:\n",
    "        assert (z_trg is None) != (x_ref is None)\n",
    "    # with real images\n",
    "    x_real.requires_grad_()\n",
    "    out = nets.discriminator(x_real, masks, y_org)\n",
    "    loss_real = adv_loss(out, 1)\n",
    "    loss_reg = r1_reg(out, x_real)\n",
    "\n",
    "    # with fake images\n",
    "    with torch.no_grad():\n",
    "        x_mask = x_real * (1 - masks)\n",
    "        x_p_mask = x_p * (1-masks)\n",
    "        if pre:\n",
    "            x_fake = nets.generator(x_mask, None, x_p_mask, masks)\n",
    "        else:\n",
    "            if z_trg is not None:\n",
    "                s_trg = nets.mapping_network(z_trg, y_trg)\n",
    "            else:  # x_ref is not None\n",
    "                s_trg = nets.style_encoder(x_ref, y_trg)\n",
    "            x_fake = nets.generator(x_real, s_trg, masks=masks)\n",
    "    out = nets.discriminator(x_fake, masks, y_trg)\n",
    "    loss_fake = adv_loss(out, 0)\n",
    "\n",
    "    loss = loss_real + loss_fake + args.lambda_reg * loss_reg\n",
    "    return loss, Munch(real=loss_real.item(),\n",
    "                       fake=loss_fake.item(),\n",
    "                       reg=loss_reg.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "251fe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real.requires_grad_()\n",
    "out = nets.discriminator(x_real, mask, y_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fcbd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.full_like(out, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58f20fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fa05850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4137, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b74d2d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8252, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(out, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_g_loss_pre(nets, loss_net, args, x_real, x_p, y_org, x_ref=None, masks=None):\n",
    "    x_mask = x_real * (1-masks)\n",
    "    x_p_mask = x_p * (1 - masks)\n",
    "    first_out, x_fake = nets.generator(x_mask, None, x_p_mask, masks=masks)\n",
    "    out = nets.discriminator(x_fake, masks, y_org)\n",
    "    loss_adv = adv_loss(out, 1)\n",
    "\n",
    "    # style reconstruction loss\n",
    "    if x_ref is not None:\n",
    "        s_trg = nets.style_encoder(x_ref, y_org)\n",
    "        s_pred = nets.style_encoder(x_fake, y_org)\n",
    "        loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n",
    "    else:\n",
    "        loss_sty = 0\n",
    "    # L1 Loss\n",
    "    parsing_L1Loss = F.l1_loss(first_out, x_p)\n",
    "    out_L1Loss = F.l1_loss(out, x_real)\n",
    "\n",
    "    # perceptual loss and style loss\n",
    "    with torch.no_grad():\n",
    "        perceptual_loss = loss_net.Perceptual(out, x_real)\n",
    "        style_loss = loss_net.Style(out * masks, x_real * masks)\n",
    "\n",
    "    loss = loss_adv + args.lambda_sty * loss_sty + args.lambda_pl1 * parsing_L1Loss + args.lambda_il1 * out_L1Loss +\\\n",
    "           args.p_loss * perceptual_loss + args.s_loss * style_loss\n",
    "\n",
    "    return loss, Munch(adv=loss_adv.item(),\n",
    "                       sty=loss_sty.item(),\n",
    "                       ps=parsing_L1Loss.item(),\n",
    "                       cyc=out_L1Loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43905b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
