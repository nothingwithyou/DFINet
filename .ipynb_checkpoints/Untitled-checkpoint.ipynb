{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497dc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "from munch import Munch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import argparse\n",
    "from core.data_loader import get_train_loader\n",
    "from core.data_loader import get_test_loader\n",
    "from core.solver import Solver, compute_d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efd33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# model arguments\n",
    "parser.add_argument('--img_size', type=int, default=256,\n",
    "                    help='Image resolution')\n",
    "parser.add_argument('--num_domains', type=int, default=2,\n",
    "                    help='Number of domains')\n",
    "parser.add_argument('--latent_dim', type=int, default=16,\n",
    "                    help='Latent vector dimension')\n",
    "parser.add_argument('--latent_channels', type=int, default=64,\n",
    "                    help='latent channels for coarse')\n",
    "parser.add_argument('--norm', type=str, default='in',\n",
    "                    help='normalization type for coarse')\n",
    "parser.add_argument('--pad_type', type=str, default='zero',\n",
    "                    help='the padding type for coarse')\n",
    "parser.add_argument('--activation', type=str, default='elu',\n",
    "                    help='the activation type for coarse')  # elu\n",
    "parser.add_argument('--hidden_dim', type=int, default=512,\n",
    "                    help='Hidden dimension of mapping network')\n",
    "parser.add_argument('--style_dim', type=int, default=64,\n",
    "                    help='Style code dimension')\n",
    "\n",
    "# weight for objective functions\n",
    "parser.add_argument('--lambda_reg', type=float, default=1,\n",
    "                    help='Weight for R1 regularization')\n",
    "parser.add_argument('--lambda_cyc', type=float, default=1,\n",
    "                    help='Weight for cyclic consistency loss')\n",
    "parser.add_argument('--lambda_sty', type=float, default=1,\n",
    "                    help='Weight for style reconstruction loss')\n",
    "parser.add_argument('--lambda_ds', type=float, default=1,\n",
    "                    help='Weight for diversity sensitive loss')\n",
    "parser.add_argument('--lambda_pl1', type=float, default=1, help='the parameter of parsing L1Loss')\n",
    "parser.add_argument('--lambda_il1', type=float, default=1, help='the parameter of image L1Loss')\n",
    "parser.add_argument('--lambda_perceptual', type=float, default=2,\n",
    "                    help='the parameter of FML1Loss (perceptual loss)')\n",
    "parser.add_argument('--lambda_gan', type=float, default=1,\n",
    "                    help='the parameter of valid loss of AdaReconL1Loss; 0 is recommended')\n",
    "parser.add_argument('--s_loss', type=float, default=2.5, help='STYLE_LOSS_WEIGHT')\n",
    "parser.add_argument('--p_loss', type=float, default=0.1, help='CONTENT_LOSS_WEIGHT')\n",
    "parser.add_argument('--ds_iter', type=int, default=100000,\n",
    "                    help='Number of iterations to optimize diversity sensitive loss')\n",
    "parser.add_argument('--w_hpf', type=float, default=1,\n",
    "                    help='weight for high-pass filtering')\n",
    "\n",
    "# training arguments\n",
    "parser.add_argument('--randcrop_prob', type=float, default=0.5,\n",
    "                    help='Probabilty of using random-resized cropping')\n",
    "parser.add_argument('--total_iters', type=int, default=100000,\n",
    "                    help='Number of total iterations')\n",
    "parser.add_argument('--resume_iter', type=int, default=0,\n",
    "                    help='Iterations to resume training/testing')\n",
    "parser.add_argument('--batch_size', type=int, default=8,\n",
    "                    help='Batch size for training')\n",
    "parser.add_argument('--val_batch_size', type=int, default=32,\n",
    "                    help='Batch size for validation')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='Learning rate for D, E and G')\n",
    "parser.add_argument('--f_lr', type=float, default=1e-6,\n",
    "                    help='Learning rate for F')\n",
    "parser.add_argument('--beta1', type=float, default=0.0,\n",
    "                    help='Decay rate for 1st moment of Adam')\n",
    "parser.add_argument('--beta2', type=float, default=0.99,\n",
    "                    help='Decay rate for 2nd moment of Adam')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                    help='Weight decay for optimizer')\n",
    "parser.add_argument('--num_outs_per_domain', type=int, default=10,\n",
    "                    help='Number of generated images per domain during sampling')\n",
    "\n",
    "# misc\n",
    "parser.add_argument('--mode', type=str, required=True,\n",
    "                    choices=['train', 'eval', 'sample'],\n",
    "                    help='This argument is used in solver')\n",
    "parser.add_argument('--num_workers', type=int, default=4,\n",
    "                    help='Number of workers used in DataLoader')\n",
    "parser.add_argument('--seed', type=int, default=777,\n",
    "                    help='Seed for random number generator')\n",
    "\n",
    "# directory for training\n",
    "parser.add_argument('--train_img_dir', type=str, default='data/celeba_hq/train',\n",
    "                    help='Directory containing training images')\n",
    "parser.add_argument('--val_img_dir', type=str, default='data/celeba_hq/val',\n",
    "                    help='Directory containing validation images')\n",
    "parser.add_argument('--sample_dir', type=str, default='expr/samples',\n",
    "                    help='Directory for saving generated images')\n",
    "parser.add_argument('--checkpoint_dir', type=str, default='expr/checkpoints',\n",
    "                    help='Directory for saving network checkpoints')\n",
    "\n",
    "# directory for calculating metrics\n",
    "parser.add_argument('--eval_dir', type=str, default='expr/eval',\n",
    "                    help='Directory for saving metrics, i.e., FID and LPIPS')\n",
    "\n",
    "# directory for testing\n",
    "parser.add_argument('--result_dir', type=str, default='expr/results',\n",
    "                    help='Directory for saving generated images and videos')\n",
    "parser.add_argument('--src_dir', type=str, default='assets/representative/celeba_hq/src',\n",
    "                    help='Directory containing input source images')\n",
    "parser.add_argument('--ref_dir', type=str, default='assets/representative/celeba_hq/ref',\n",
    "                    help='Directory containing input reference images')\n",
    "parser.add_argument('--inp_dir', type=str, default='assets/representative/custom/female',\n",
    "                    help='input directory when aligning faces')\n",
    "parser.add_argument('--out_dir', type=str, default='assets/representative/celeba_hq/src/female',\n",
    "                    help='output directory when aligning faces')\n",
    "\n",
    "# face alignment\n",
    "parser.add_argument('--wing_path', type=str, default='expr/checkpoints/wing.ckpt')\n",
    "parser.add_argument('--lm_path', type=str, default='expr/checkpoints/celeba_lm_mean.npz')\n",
    "\n",
    "# step size\n",
    "parser.add_argument('--print_every', type=int, default=10)\n",
    "parser.add_argument('--sample_every', type=int, default=5000)\n",
    "parser.add_argument('--save_every', type=int, default=10000)\n",
    "parser.add_argument('--eval_every', type=int, default=50000)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "args = parser.parse_args(['--mode','train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37891f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataLoader to fetch source images during the training phase...\n",
      "Preparing DataLoader for the generation phase...\n"
     ]
    }
   ],
   "source": [
    "pre_loaders = Munch(src=get_train_loader(root=args.train_img_dir,\n",
    "                                             which='source',\n",
    "                                             img_size=args.img_size,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             prob=args.randcrop_prob,\n",
    "                                             num_workers=args.num_workers,\n",
    "                                             pre=True),\n",
    "                        val=get_test_loader(root=args.val_img_dir,\n",
    "                                            img_size=args.img_size,\n",
    "                                            batch_size=args.val_batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ee9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_loader import InputFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c534527",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = InputFetcher(pre_loaders.src, latent_dim=args.latent_dim, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00197c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher_val = InputFetcher(pre_loaders.val, None, latent_dim=args.latent_dim, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7972774",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_val = next(fetcher_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da75777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14051e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = build_model(args,pre=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ebdb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real, x_p, y_org, mask = inputs_val.x, inputs_val.p, inputs_val.y, inputs_val.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79087cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50614e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_d_loss(nets, args, x_real, x_p, y_org, y_trg=None, z_trg=None, x_ref=None, masks=None, pre=False):\n",
    "    if not pre:\n",
    "        assert (z_trg is None) != (x_ref is None)\n",
    "    # with real images\n",
    "    x_real.requires_grad_()\n",
    "    out = nets.discriminator(x_real, masks, y_org)\n",
    "    loss_real = adv_loss(out, 1)\n",
    "    loss_reg = r1_reg(out, x_real)\n",
    "\n",
    "    # with fake images\n",
    "    with torch.no_grad():\n",
    "        x_mask = x_real * (1 - masks)\n",
    "        x_p_mask = x_p * (1-masks)\n",
    "        if pre:\n",
    "            x_fake = nets.generator(x_mask, None, x_p_mask, masks)\n",
    "        else:\n",
    "            if z_trg is not None:\n",
    "                s_trg = nets.mapping_network(z_trg, y_trg)\n",
    "            else:  # x_ref is not None\n",
    "                s_trg = nets.style_encoder(x_ref, y_trg)\n",
    "            x_fake = nets.generator(x_real, s_trg, masks=masks)\n",
    "    out = nets.discriminator(x_fake, masks, y_trg)\n",
    "    loss_fake = adv_loss(out, 0)\n",
    "\n",
    "    loss = loss_real + loss_fake + args.lambda_reg * loss_reg\n",
    "    return loss, Munch(real=loss_real.item(),\n",
    "                       fake=loss_fake.item(),\n",
    "                       reg=loss_reg.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "251fe736",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_real.requires_grad_()\n",
    "out = nets.discriminator(x_real, mask, y_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fcbd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.full_like(out, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58f20fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fa05850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4137, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b74d2d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8252, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(out, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_g_loss_pre(nets, args, x_real, x_p, y_org, x_ref=None, masks=None):\n",
    "    x_mask = x_real * (1-masks)\n",
    "    x_p_mask = x_p * (1 - masks)\n",
    "    x_fake = nets.generator(x_mask, None, x_p_mask, masks=masks)\n",
    "    out = nets.discriminator(x_fake, masks, y_org)\n",
    "    loss_adv = adv_loss(out, 1)\n",
    "\n",
    "    # style reconstruction loss\n",
    "    if x_ref is not None:\n",
    "        s_trg = nets.style_encoder(x_ref, y_org)\n",
    "        s_pred = nets.style_encoder(x_fake, y_org)\n",
    "        loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n",
    "    else:\n",
    "        loss_sty = 0\n",
    "    loss = loss_adv + args.lambda_sty * loss_sty\n",
    "    return loss, Munch(adv=loss_adv.item(),\n",
    "                       sty=loss_sty.item(),\n",
    "                       ds=loss_ds.item(),\n",
    "                       cyc=loss_cyc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
